{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bba82",
   "metadata": {},
   "source": [
    "## <center> Assignment 3 </center>\n",
    "\n",
    "#### Name: Aditya Chauhan\n",
    "#### Student ID: 169027493\n",
    "\n",
    "You are provided with a training dataset and a testing dataset for a binary classification problem with labels {0, 1}. The last column of the training dataset contains the labels, while the testing dataset includes only attributes (descriptive features).\n",
    "\n",
    "Train an effective classifier using the training dataset. You may choose your data processing approach, classifier type, and parameter tuning methods as needed. The sklearn package in Python is recommended for implementing your model.\n",
    "\n",
    "Make predictions on the testing dataset and generate a file containing a single column of predicted labels (0 or 1) in the same order as the testing dataset. Ensure that the output file does not include a header and that your prediction.txt file contains exactly one column and 352 rows.\n",
    "\n",
    "Please submit your implementation code and the predicted output file as two separate files (not compressed into a zip file), named <b>A3.ipynb</b> and <b>prediction.txt</b>, respectively. Your assignment will be evaluated based on your model's performance, particularly its F1-score, as well as other criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2002f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('A3_data/train.csv',sep=',',index_col=0) \n",
    "df_test_attribute_only = pd.read_csv('A3_data/test_attribute.csv',sep=',',index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b2a48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 652 entries, 0 to 651\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       652 non-null    float64\n",
      " 1   1       652 non-null    float64\n",
      " 2   2       652 non-null    float64\n",
      " 3   3       652 non-null    float64\n",
      " 4   4       652 non-null    float64\n",
      " 5   5       652 non-null    float64\n",
      " 6   6       652 non-null    float64\n",
      " 7   7       652 non-null    float64\n",
      " 8   8       652 non-null    int64  \n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 50.9 KB\n",
      "None\n",
      "\n",
      "Test Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 352 entries, 0 to 351\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       352 non-null    float64\n",
      " 1   1       352 non-null    float64\n",
      " 2   2       352 non-null    float64\n",
      " 3   3       352 non-null    float64\n",
      " 4   4       352 non-null    float64\n",
      " 5   5       352 non-null    float64\n",
      " 6   6       352 non-null    float64\n",
      " 7   7       352 non-null    float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 24.8 KB\n",
      "None\n",
      "\n",
      "Train Dataset Head:\n",
      "      0     1     2     3    4    5     6     7  8\n",
      "0  0.81  0.85  0.47  0.37  0.5  0.0  0.56  0.22  1\n",
      "1  0.70  0.58  0.53  0.39  0.5  0.0  0.59  0.22  1\n",
      "2  0.72  0.73  0.41  0.28  0.5  0.0  0.44  0.22  1\n",
      "3  0.78  0.69  0.44  0.26  0.5  0.0  0.54  0.22  1\n",
      "4  0.74  0.82  0.46  0.24  0.5  0.0  0.48  0.22  1\n",
      "\n",
      "Test Dataset Head:\n",
      "      0     1     2     3    4     5     6     7\n",
      "0  0.74  0.72  0.50  0.28  0.5  0.00  0.49  0.27\n",
      "1  0.80  0.88  0.36  0.39  0.5  0.00  0.56  0.33\n",
      "2  0.57  0.52  0.46  0.20  0.5  0.83  0.52  0.41\n",
      "3  0.77  0.82  0.40  0.36  0.5  0.00  0.38  0.22\n",
      "4  0.69  0.60  0.51  0.13  0.5  0.83  0.52  0.22\n",
      "\n",
      "Missing Values in Train Dataset:\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "dtype: int64\n",
      "\n",
      "Missing Values in Test Dataset:\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "dtype: int64\n",
      "\n",
      "F1 Score on Validation Set: 0.8571428571428571\n",
      "\n",
      "Predictions saved to prediction.txt\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Train Dataset Info:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nTest Dataset Info:\")\n",
    "print(df_test_attribute_only.info())\n",
    "\n",
    "print(\"\\nTrain Dataset Head:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\nTest Dataset Head:\")\n",
    "print(df_test_attribute_only.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Train Dataset:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values in Test Dataset:\")\n",
    "print(df_test_attribute_only.isnull().sum())\n",
    "\n",
    "# Correlation Heatmap\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(df_train.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "# plt.title(\"Correlation Heatmap\")\n",
    "# plt.show()\n",
    "\n",
    "X_train = df_train.iloc[:, :-1]\n",
    "y_train = df_train.iloc[:, -1]\n",
    "\n",
    "# Handle missing values & scale\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(df_test_attribute_only)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "## Train k-NN Model\n",
    "\n",
    "# Split training data into train + validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train k-NN Classifier & Validate the Model\n",
    "# Can change the n_neighbors value to change F-1 Score\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_split, y_train_split)\n",
    "y_val_pred = knn.predict(X_val)\n",
    "print(\"\\nF1 Score on Validation Set:\", f1_score(y_val, y_val_pred))\n",
    "\n",
    "# Predictions on Test Data\n",
    "y_test_pred = knn.predict(X_test)\n",
    "output_file = 'prediction.txt'\n",
    "np.savetxt(output_file, y_test_pred.astype(int), fmt='%d')\n",
    "print(f\"\\nPredictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea59e06",
   "metadata": {},
   "source": [
    "### Briefly describe your approach in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c942ae",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before building the model, I wanted to explore & understand the data to identify potential issues:\n",
    "\n",
    "- Loaded & Checked dataset info\n",
    "- Identified & handled missing values\n",
    "- Used heatmap to visualize relationships between features\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "- Missing Value imputation, ensuring nothing is discrarded\n",
    "- Applied StandardScaler to standardize the features\n",
    "    - Ensured testing & training set both had the same standardization\n",
    "- Seperated features from target in the training dataset\n",
    "    - Only test data used for predictions\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "k-NN was chosen because it very well categorizes data, it is simple, & it captures local pattens, which compliments the dataset. Since we're trying to categorize points as either (0,1) I felt that similarity-based learning was the best approach.  \n",
    "\n",
    "## Model Implementation\n",
    "\n",
    "- Used scikit-learn for Euclidean distance\n",
    "- Sorted training points by distance for neighbor selection\n",
    "- Selected k-closest points\n",
    "    - If there is a tie, the lower-index neighbors are prioritized\n",
    "- Predictions saved in output\n",
    "\n",
    "## Model Training\n",
    "\n",
    "- Final k-NN model was trained using the optimal k-value, found by trial and error till it was \"optimized\"\n",
    "- Used a training-validation split (80/20) to evaluate on unseen validation data\n",
    "- Performance validated by F-1 Score\n",
    "- Tried to use cross validation to find optimum k-value (Didn't work)\n",
    "\n",
    "## Advantages & Disadvantages\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Simple\n",
    "- Flexible\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Incredibly sensitive to scaling & k value\n",
    "- Performance gets worse with more dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
